<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>مقدمة في ترميز النصوص  AI and Machine Learning Blog</title>
<meta name="description" content="">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="AI and Machine Learning Blog">
<meta property="og:title" content="مقدمة في ترميز النصوص">
<meta property="og:url" content="http://localhost:4000/data_sampling/">


  <meta property="og:description" content="">







  <meta property="article:published_time" content="2020-06-11T00:00:00-04:00">





  

  


<link rel="canonical" href="http://localhost:4000/data_sampling/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Mohammed Alhamid",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="AI and Machine Learning Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single_ar">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">AI and Machine Learning Blog</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/blog" >Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/about" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/ar_home" >مقالات</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main" >
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/alhamid-profile.jpg" alt="Mohammed Alhamid" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Mohammed Alhamid</h3>
    
    
      <p class="author__bio" itemprop="description">
        PhD, Machine Learning and AI
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://medium.com/@moh.alhamid" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-medium" aria-hidden="true"></i> medium</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="مقدمة في ترميز النصوص">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="June 11, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline"><div class="col-md-6 text-right">مقدمة في ترميز النصوص</div>
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text" >
        
        <p><img src="/assets/images/126-icon.png" alt="" /></p>

<div style="direction: rtl; text-align: right;">
كما هو من البديهي، فإن الآلة لا تتعامل مع النصوص بحروفها وأفعالها وأسمائها وأنه يتوجب علينا تحويل هذه النصوص لمعاملات رقمية يمكن للآلة أن تبني خوارزمياتها عليها. بناءً على ذلك، كيف للآلة أن تتعلم أن نصاً معيناً يحمل صفةً إيجابية مثلاً أو سلبية؟ كيف لمحركات البحث أن تفهم عن ماذا نبحث عند ادخالنا لمجموعة من النصوص في حقل الادخال. كل ذلك مبني على مفهوم بسيط يطلق عليه الترميز. 
<br /><br />
تعلم الآلة دائماً ما يبتدئ ببناء قناة تتدفق منها البيانات عبر مراحل حتى الوصول من خلالها لبناء النموذج. هذه البيانات تحتاج منا لتحويل هذه النصوص لمعاملات رقمية. عملية الترميز هذه تمكننا من بناء نماذج لتعلم الآلة بنفس الآلية لمثيلات هذه البيانات من البيانات الرقمية الأصيلة كمجموع المبيعات وعدد القطع والأوزان وغيرها. تعرف عملية الترميز بمصطلح "Text Encoding".
<br /><br />
التقنيات المستخدمة اليوم لترميز النصوص ليست بالمعقدة إطلاقاً وسنستعرض سوية مجموعة منها مع الأمثلة باستخدام لغة بايثون بالتفصيل. سنستخدم في هذا الشرح مجموعة من النصوص استخرجناها من موسوعة ويكيبيديا تتكلم عن عدد الأشجار الموجودة حول العالم.
<br /><br />
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>corpus = [
	"The number of trees in the world, according to a 2015 estimate, is 3.04 trillion.",
	"46% of the trees in the world are in the tropics or sub-tropics.",
	"20% of the trees in the world are in the temperate zones.",
	"24% of threes in the world are in the coniferous boreal forests.",
	"There about 15 billion trees are cut down annually.",
	"There about 5 billion trees are planted annually.",
]
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
<h2>استخراج المميزات من النصوص</h2>
نماذج تعلم الآلة تتعلم من المميزات الوصول لأهداف التعلم. هذه المميزات يطلق عليها (Features) هي خلاص تنقيحنا للبيانات واستخراج ما نظن أنه مفيد لتعلم الآلة. هذا في حال كان التعلم عن طريق الاشراف حيث نزود النموذج ببيانات التدريب والعناصر المستهدفة وهذا ما نستهدفه هنا في هذه المقالة. 
<br /><br />
استخراج المميزات يمر بأربع مراحل أساسية بالإمكان الاستزادة عليها حسب سياق المشكلة ونوع الهدف المراد للآلة أن تستهدفه. الخطوة الأولى تهدف لاستخلاص الكلمات من خلال فصلها عن الجمل. تلي هذه الخطوة إزالة المستبعدات وهي تلك الكلمات نستبعدها من سلسلة الكلمات المستخلصة ليسهل علينا معالجة اللغة. يمكننا تحديد هذه المستبعدات عن طريق الانتباه لكثرة تكرارها كحروف الجر (مثلاً: في، على، من، إلى) وحروف العطف والاستفهام والشرط. لا يوجد تعريف واضح لهذه المستبعدات ولا قائمة بالكلمات التي تشملها سواءً باللغة الإنجليزية أو العربية لعدم وجود توافق علمي شامل عما يمكننا استبعاده. على الرغم من ذلك فهناك مكتبات توفر قائمتها من هذه المستبعدات يمكن استخدامها بسهولة. ولتبسيط المفهوم، فيمكننا اعتبار الكلمات المستبعدة هي كل تلك الكلمات التي لا تحمل معنى بصفتها الفردية و التي تحتاج لوضعها في سياق أو مضافة لكلمات أخرى. أول من اقترح إزالة هذه الكلمات هو العالم (Hans Peter Luhn) في عام ١٩٥٩ م. 
<br /><br />
بعد استخلاص الكلمات من النصوص نحتاج لاستعادة جذور هذه الكلمات وبذلك تصل كل كلمة إلى جذرها. بالنسبة للغات التي تُشكل في أشكال حروف كبيرة وأخرى صغيرة كاللغة الإنجليزية، فيتوجب علينا استخدام نمط واحد لشكل هذه الحروف، أي تحويل جميع الحروف لحروف صغيرة، مثلاً. أما بالنسبة للغة العربية فيتوجب علينا الانتباه لتصحيح الكلمات التي تكتب خطأً بالهاء أو التاء المربوطة مثل: مكتبة لتصحيح مكتبه، وهكذا. الخطوة الرابعة هي إزالة علامات الترقيم والأحرف الخاصة والكلمات التي يبلغ طولها حرفاً واحداً. 
<br /><br />

<h3> استخراج الكلمات </h3>

يمكن استخراج الكلمات باستخدام الأكواد المذيلة بالأسفل مع العمل أن مكتبة (Keras) تقدم واجهات (API) تقوم بهذه 
العملية.
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Extracting the tokens using NLTK
# We need to serialize the text to splits the works into tokens
nltk_tokens = [] 
for text in corpus:
    pre_text = pd.Series(text).str.cat(sep=' ')
    nltk_tokens.append(word_tokenize(pre_text))
print(nltk_tokens)
</code></pre></div></div>
<div style="direction: rtl; text-align: right;">
وهنا باستخدام المكتبة: 
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Extracting the tokens using Keras
ks_tokens = []
for text in corpus: 
    ks_tokens.append(keras_text_to_word_sequence(text))
print(ks_tokens)
</code></pre></div></div>
<div style="direction: rtl; text-align: right;">
لو أخذنا أول جملة في النص الذي اخترناه من موسوعة ويكيبيديا، فإن مخرجات هذ الخطوة عبارة عن متجه يحتوي على الكلمات كما في الصورة التالية:

<img src="/assets/images/127-icon.png" alt="" />
<br />
<h3> تهيئة الكلمات المستخرجة </h3>
تهيئة الكلمات تشمل الخطوات التي ذكرناها سابقاً والتي تشمل إزالة المستبعدات و استعادة جذور الكمات و إزالة علامات الترقيم و الأحرف الخاصة و الرموز المكونة من حرف واحد. 
بالامكان تحميل الكلمات المستبعدة في اللغة الانجليزية من المكتبة الأكثر شيوعاً في معالجة اللغة الطبيعية (nltk) على النحو التالي:
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from nltk.corpus import stopwords
# Retriving the stop words from the library 
stop_words = set(stopwords.words('english'))

for i in range(0, len(ks_tokens)): 
    ks_tokens[i] = [w for w in pd.Series(ks_tokens[i]) if not w in stop_words]
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
من أمثلة الكلمات المستبعدة، الكلمات التالية: ['on', 'other', 'of', 'him', 'didn', 'have', 'haven', 'or', 'won', 'couldn']. نتاج هذه الخطوة على متجه الكلمات للجملة الأولى توضحه الصورة التالية:
<img src="/assets/images/128-icon.png" alt="" />
<br />
يتطلب علينا الآن استعادة جذور الكلمات و سننفذ هذه الخطوة على النصوص الانجليزية باستخدام خوارزمية (Porter) على النحو التالي: 
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># We will use Porter algorithm to reduce the words. Porter has 5 phases of word reductions: 
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
لن نفصل في طريقة عمل هذه الخوارزمية، لكن لو أخذنا مثال كلمة (Eating) فإن الخورازمية تعمل على ازالة اللواحق لأصل الكلمة و تردها للجذر (Eat).
علينا الآن المرور على الكلمات المستخرجة و استعادة جذورها: 
</div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for tokens in ks_tokens:
    for i in range(0, len(tokens)):    
        tokens[i] = porter.stem(tokens[i])
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">

أصبح المتجه المستخلص من الجملة الأولى من النص بهذا الشكل: 
<img src="/assets/images/129-icon.png" alt="" />
<br />
الأكواد التالية ستعمل على إزالة الأرقام و علامات الترقيم و الأحرف الخاصة كما تقدم: 
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i in range(0, len(ks_tokens)):
    new_ks_tokens = []
    for tok in ks_tokens[i]: 
        tok = tok.translate(str.maketrans('', '', string.punctuation))
        if tok != "" and len(tok) &gt; 1 and (tok.isnumeric()==False):
            new_ks_tokens.append(tok)
    ks_tokens[i] = new_ks_tokens
</code></pre></div></div>
<div style="direction: rtl; text-align: right;">
<br />
وهكذا يصبح شكل المتجه النهائي بعد تهيئة هذه النصوص على هذا النحو: 
<img src="/assets/images/130-icon.png" alt="" />
<h2>ترميز النصوص</h2>

تحتوي الخطوات السابقة على العديد من الحلقات المتكررة ويمكن بسهولة أن تكون مكلفة للغاية خصوصاً إذا زاد عدد الكلمات بشكل كبير. وبالتالي ، سوف نكرر الخطوات السابقة باستخدام أسطر قليلة من التعليمات البرمجية باستخدام مكتبة (Keras):
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from keras.preprocessing.text import Tokenizer
# First step is to get an instance of the tokenizer class
tok = Tokenizer()
# Keras can fit our corpus in a single call
tok.fit_on_texts(corpus)
</code></pre></div></div>
<div style="direction: rtl; text-align: right;">
بالامكان الآن استطلاع تردد كل كلمة في المتجهات و عدد تكرارها في كل جملة على النحو التالي: 
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Let's explore what is the content of the tokenized corpus
print('The frequency of each word in the corpus:')
print(tok.word_counts)
</code></pre></div></div>

<p><img src="/assets/images/131-icon.png" alt="" /></p>
<div style="direction: rtl; text-align: right;">
نستطلع فيما تبقى من هذه المقالة، طرق الترميز لمتجهات الكلمات بعد تنقيحها و معالجتها.
<h3>الترميز الساخن </h3>
يقصد بالترميز الساخن بناء متجهات تحمل جميع المفردات الموجودة في القاموس النصي المدخل ويكون حجم المتجه مساو لعدد الكلمات المستخرجة. يحتوي عناصر كل متجه على قيم تساوي الصفر أو الواحد. عدد المتجهات يساوي عدد الجمل النصية المدخلة. سمي بالساخن نظراً أنه يشغل العناصر التي تظهر في الجملة بقيمة ١ عند ظهور الكلمة كما في المثال التالي:
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(tok.texts_to_matrix(corpus, mode='binary'))
</code></pre></div></div>

<p><img src="/assets/images/132-icon.png" alt="" /></p>
<div style="direction: rtl; text-align: right;">
الصورة التالية توضح ترميز الجملة الأولى على حسب ظهور الكلمات في القاموس. 
<img src="/assets/images/133-icon.png" alt="" />
يرجى التنبه أن طريقة الترميز هذه افقدتنا ترتيب الكلمات، فنستطيع أن نتعلم من المتجه وجود الكلمة من عدمها لكن دون ترتيب الظهور.
<h3> الترميز العددي </h3>
هذا النوع من الترميز مشابه لسابقه إلا أنه عوضاً عن استخدام القيمة ١، نستخدم تردد الكلمة في نفس الجملة.
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Encoding the frequency of each word in the sentence
print(tok.texts_to_matrix(corpus, mode='count'))
</code></pre></div></div>

<p><img src="/assets/images/134-icon.png" alt="" /></p>
<div style="direction: rtl; text-align: right;">
يستخدم لوصف هذا النوع من الترميز مصطلح ([Frequency-based vectorization) وهناك طرق مشابهة للترميز العددي أو الترددي أشهرها (TF‪-‬IDF). ويمكن حساب الترميز بهذه الطريقة بناء على عدد الظهور لكلمة معينة على اجمالي عدد الكلمات في المستند. 
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Encoding the frequency of each word in the sentence (TFIDF)
print(tok.texts_to_matrix(corpus, mode='tfidf'))
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
<h3> استخدام نموذج لتضمين الكلمات </h3>
في هذه النوع من الترميز، نعول على تعلم الآلة في بناء نموذج لحساب دلالة الكلمة بدلاً من ترميز ظهورها في الجملة. هذه الطريقة تعرف بمصطلح (Word Embedding). يتم بناء متجهات الكلمات بناءً على المسافة بين كل متجه وآخر. لبناء هذا النموذج، نستخدم شبكة عصبية لتتعلم ربط هذه المتجهات لبناء دلالة ضمنية تتنبأ بالكلمات المحيطة بها.
<br /><br />
تكرر للتو في الفقرة السابقة كلمة دلالة في أكثر من موضع، فما الدلالة التي نبحث عنها؟ الدلالة المقصودة هنا هي كيف لنا أن نعرف أن كلمتين مختلفتين في الحروف تتشابهان في المعنى؟ كلمة "تعلم"و كلمة "دراسة" كلاهما يحملان نفس الدلالة لكن كل كلمة منهما لا تشتركان في الحروف و لا في الأطوال. دلالة المعنى في اللغة الطبيعية تأتي في كيفية استخدام الكلمات في السياق وكيف تستخدم الكلمة مع نضيراتها في نفس الجملة. سنتطرق لكيفية بناء هذه النماذج التي تتنبأ بضمنية الكلمة ثم نستخدم بعض النماذج المدربة مسبقاً.
<br /><br />
يوجد نموذجان واسعا الانتشار الأول Word2Vec و الآخر GloVe. لبناء نموذج محاكي لطريقة بناء متجهات Word2Vec نحتاج المكتبات التالية للتسهيل:
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">gensim</span> 
<span class="k">from</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span> <span class="n">import</span> <span class="n">Word2Vec</span>

<span class="p">#</span> <span class="n">Building</span> <span class="n">a</span> <span class="n">word2vec</span> <span class="k">model</span> <span class="n">typically</span> <span class="n">is</span> <span class="n">very</span> <span class="n">expensive</span><span class="p">,</span> <span class="n">so</span> <span class="n">we</span> <span class="n">need</span> <span class="k">to</span> <span class="n">keep</span> <span class="k">in</span> <span class="n">mind</span> <span class="n">the</span> <span class="n">multiprocessing</span> 
<span class="p">#</span> <span class="n">approach</span> <span class="n">option</span>
<span class="n">import</span> <span class="n">multiprocessing</span> <span class="n">cores</span> <span class="p">=</span> <span class="n">multiprocessing</span><span class="p">.</span><span class="n">cpu_count</span><span class="p">()</span>

<span class="p">#</span> <span class="k">Parameters</span>
<span class="p">#</span> <span class="n">size</span> <span class="p">(</span><span class="n">int</span><span class="p">,</span> <span class="n">optional</span><span class="p">)</span> <span class="err">–</span> <span class="n">Dimensionality</span> <span class="k">of</span> <span class="n">the</span> <span class="n">word</span> <span class="n">vectors</span><span class="p">.</span>
<span class="p">#</span> <span class="n">window</span> <span class="p">(</span><span class="n">int</span><span class="p">,</span> <span class="n">optional</span><span class="p">)</span> <span class="err">–</span> <span class="n">Maximum</span> <span class="n">distance</span> <span class="n">between</span> <span class="n">the</span> <span class="n">current</span> <span class="k">and</span> <span class="n">predicted</span> <span class="n">word</span> <span class="n">within</span> <span class="n">a</span> <span class="n">sentence</span><span class="p">.</span>
<span class="p">#</span> <span class="n">min_count</span> <span class="p">(</span><span class="n">int</span><span class="p">,</span> <span class="n">optional</span><span class="p">)</span> <span class="err">–</span> <span class="n">Ignores</span> <span class="n">all</span> <span class="n">words</span> <span class="k">with</span> <span class="n">total</span> <span class="n">frequency</span> <span class="n">lower</span> <span class="n">than</span> <span class="n">this</span><span class="p">.</span>
<span class="p">#</span> <span class="n">workers</span> <span class="p">(</span><span class="n">int</span><span class="p">,</span> <span class="n">optional</span><span class="p">)</span> <span class="err">–</span> <span class="n">Use</span> <span class="n">these</span> <span class="n">many</span> <span class="n">worker</span> <span class="n">threads</span> <span class="k">to</span> <span class="n">train</span> <span class="n">the</span> <span class="k">model</span> <span class="p">(=</span><span class="n">faster</span> <span class="n">training</span> <span class="k">with</span> 	<span class="n">multicore</span> <span class="n">machines</span><span class="p">).</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Word2Vec</span> <span class="p">(</span><span class="n">ks_tokens</span><span class="p">,</span> <span class="n">window</span><span class="p">=</span><span class="m">10</span><span class="p">,</span> <span class="n">size</span><span class="p">=</span><span class="m">150</span><span class="p">,</span> <span class="n">min_count</span><span class="p">=</span><span class="m">1</span><span class="p">,</span><span class="n">workers</span><span class="p">=</span><span class="m">10</span><span class="p">)</span>
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
الرسم التفاعلي التالي يرسم بشكل مقتطع طريقة بناء هذا النموذج حيث تُبنى شبكة عصبية من طبقة مخفية واحدة. هذا النموذج يُمكننا بسهولة مقارنة المفردات عن طريق تعلم التفاعل الضمني المتوفر من خلال النص المدخل. 
<img src="/assets/images/138-icon.gif" alt="" />
بناء النموذج على عدد قليل من الكلمات ليس ذو قيمة، وذلك نظراً لعدم كفاية النصوص التي تمكن النموذج من تنبؤ المسافات بين كل متجه كما توضحه الصورة التالية. كما أن بناء نماذج Word2Vec مكلفة وتستغرق مساحة من الوقت و الذاكرة العشوائية عند البناء. وفي كثير من الأحيان، نلحظ أن بناء نموذج Word2Vec على قاعدة بيانات صغيرة سرعان ما ينتج عنه نموذجاً يكون مضاعفاً لحجم البيانات الأساسي. وعليه بالامكان استخدام نماذج تم تدريبها مسبقاً على بيانات أصغر بالحجم و حققت دقة عالية في بناء المتجهات الضمنية للكلمات. 
<img src="/assets/images/137-icon.png" alt="" />
<br /><br />
سنستخدم نموذج GloVe وهي خوارزمية تم تدريبها على نصوص من موسوعة وكيبيديا و متوفرة من جامعة ستانفورد على هذا <a href="https://nlp.stanford.edu/data/glove.6B.zip"> الرابط </a>. الأكواد التالية ستعمل على تحميل النموذج واستخدامه.
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
from scipy import spatial

embeddings_dict = {}
with open("models/glove.6B.50d.txt", "r") as file:
    for line in file: 
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], "float32")
        embeddings_dict[word] = vector
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
لو سألنا النموذج ماهي الكلمات القريبة من الكلمة (‪'‬book‪'‬) فستكون الاجابة مقاربة للتالي: 
['book',
 'books',
 'story',
 'biography',
 'novel',
 'writing',
 'wrote',
 'author',
 'titled',
 'published']‫.‬
الصورة التالية توضح المتجهات المتقاربة لحد ١٠٠ كلمة لكل متجه.
<img src="/assets/images/135-icon.png" alt="" />
<br />
الرسم البياني في الصورة السابقة مبني على حساب الفرق بين المسافة بين المتجهات. هناك عمليات رياضية مختلفة تُستخدم لحساب العلاقة بين المتجهات. الأكواد التالية تحسب العلاقة بين المتجهات باستخدام (Euclidean) أو (Cosine Similarity).
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Example 1 
x = embeddings_dict['book']
y = embeddings_dict['notebook']
math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))
---
5.506975291916892
</code></pre></div></div>
<div>
</div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Example 2
x = embeddings_dict['book']
y = embeddings_dict['notebook']
spatial.distance.cosine(x,y)
---
0.6004519462585449 
</code></pre></div></div>

<div style="direction: rtl; text-align: right;">
<h2> تحديات حول ترميز النصوص </h2>
<h4> الحفاظ على الترتيب </h4>
الترميز باستخدام المتجهات يستند على فهرس الكلمات الموجودة في القاموس وكما تطرقنا في المثال بداية المقال أن الترتيب في الترميز الساخن يفقدنا الحفاظ على ترتيب الكلمات في الجملة.  هناك حلول مكلفة لحل هذه المشكلة عن طريق بناء متجهات تعبر عن الجملة الواحدة بدلاً من متجه واحد لكل جملة. اختيار الترميز المناسب في هذه الحالة يعود للنموذج الذي سيتعلم من هذه النصوص فبعض النماذج بامكانها تحقيق أداء مقبول دون الحاجة للاكتراث بالترتيب.
<h4> طول المتجهات </h4>
ترميز المستندات باستخدام الفهرس مكلف إذا ما زاد عدد المفردات المميزة في المستند. زيادة هذ الأطوال ينتج عنه تمثيل معقد و متناثر لهذه المتجهات.
<h4> التحيز و فقدان الكلمات </h4>
النصوص تحمل معان تعبيرية يسهل على الانسان ادراكها من النص كمعرفة لمن يعود إليه الضمير الغائب  أو الضمير المتصل وغيرها من التحديات التي تفقد إدراكها نماذج تعلم الآلة. سرعان ما تنحاز هذه الضمائر لجنس معين في الخطاب إذا ماوجد هذا الانحياز طريقه في البيانات التي تم استخدامها لتدريب النموذج. لنأخذ مثالاً لتوضيح الفكرة. 
<img src="/assets/images/136-icon.png" alt="" />
<br />
النموذج كان منحازاً لتضمين ("مسن" أو "قديم") لكلمة "رجل" وكانت الثانية من التشابه بينما كلمة "ثمين" كانت ضمنياً أقرب لكلمة "رخيص".
<br /><br />
جميع الأكواد المستخدمة في هذه المقالة متوفرة على مستودع البيانات على هذا
‪<a href="https://github.com/malhamid/Text-Feature-Extraction/blob/master/text-encoding.ipynb">‬ الرابط‫.‬</a>
<h2> المراجع </h2>
</div>
<ul>
  <li><a href="https://nlp.stanford.edu/projects/glove/">Global Vectors for Word Representation</a></li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2020-06-11T00:00:00-04:00">June 11, 2020</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/data_sampling/" class="pagination--pager" title="تعلم الآلة ‫-‬ أساليب أخذ العينات
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/malhamid" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Mohammed Alhamid. 

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.1/js/all.js" integrity="sha384-g5uSoOSBd7KkhAMlnQILrecXvzst9TdC09/VM+pjDTCM+1il8RHz5fKANTFFb+gQ" crossorigin="anonymous"></script>








  </body>
</html>
